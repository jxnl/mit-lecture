% Student Projects Understanding Section
\newcommand{\studentunderstandingframe}{
    \section{Understanding Your Projects \& Challenges}

    \begin{frame}
        \frametitle{Overview of Your RAG \& LLM Projects}
        \begin{itemize}
            \item Most projects focus on domain-specific RAG or LLM applications
            \begin{itemize}
                \item \textbf{Finance \& Banking}: Earnings prediction, financial analysis, risk management
                \item \textbf{Operations \& Logistics}: Satellite tracking, supply chain, route optimization
                \item \textbf{Enterprise B2B}: Onboarding workflows, domain document summarization
                \item \textbf{Data Aggregation}: Legal bias detection, third-party financial data
                \item \textbf{Generative AI Products}: Assistants, code agents, advanced search
            \end{itemize}
            \item Common needs across all projects:
            \begin{itemize}
                \item Specialized search and retrieval from domain corpora
                \item Accurate question-answering with LLMs
                \item Domain customization and feedback-driven improvement
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Common Technical Challenges}
        \begin{itemize}
            \item \textbf{Data Quality \& Preparation}
            \begin{itemize}
                \item Limited labeled data for specialized domains
                \item Need for synthetic data generation and evaluation
                \item Handling diverse data types (text, tables, images, code)
            \end{itemize}
            \item \textbf{Retrieval Effectiveness}
            \begin{itemize}
                \item Domain-specific terminology and concepts
                \item Balancing precision vs. recall for specialized use cases
                \item Need for hybrid search approaches (semantic + lexical)
            \end{itemize}
            \item \textbf{Evaluation \& Improvement}
            \begin{itemize}
                \item Measuring success without large user bases
                \item Identifying failure modes systematically
                \item Creating feedback loops for continuous improvement
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Four Critical Success Factors}
        \begin{itemize}
            \item \textbf{1. Measurement-First Approach}
            \begin{itemize}
                \item Start with clear metrics before making changes
                \item Focus on retrieval recall as a leading indicator
                \item Build fast feedback loops for rapid iteration
            \end{itemize}
            \item \textbf{2. Segmentation \& Prioritization}
            \begin{itemize}
                \item Group queries by type and failure mode
                \item Focus on high-impact, high-volume segments
                \item Distinguish between data gaps and capability gaps
            \end{itemize}
            \item \textbf{3. Specialized Retrieval Methods}
            \begin{itemize}
                \item Build domain-specific indices and tools
                \item Implement effective query routing
                \item Fine-tune embeddings for your domain
            \end{itemize}
            \item \textbf{4. User-Centered Design}
            \begin{itemize}
                \item Collect meaningful feedback from day one
                \item Build trust through citations and transparency
                \item Design for perceived performance (streaming, etc.)
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Project Spotlight Examples}
        \begin{itemize}
            \item \textbf{Financial Analysis} (BlackRock, Zest AI)
            \begin{itemize}
                \item Challenge: Extracting structured data from financial documents
                \item Approach: Specialized table extraction + semantic search
                \item Evaluation: Comparison with human analyst performance
            \end{itemize}
            \item \textbf{Operational Planning} (Accenture, Maersk)
            \begin{itemize}
                \item Challenge: Complex multi-step reasoning with domain constraints
                \item Approach: Tool-based routing to specialized calculators/indices
                \item Evaluation: Plan validity and optimization metrics
            \end{itemize}
            \item \textbf{Legal/Compliance} (ICAAD, BV)
            \begin{itemize}
                \item Challenge: Detecting patterns in unstructured legal documents
                \item Approach: Fine-tuned embeddings + metadata extraction
                \item Evaluation: Comparison with manual analysis
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Today's Focus: Systematic Improvement}
        \begin{itemize}
            \item We'll cover the most relevant aspects for your current stage:
            \begin{itemize}
                \item \textbf{Evaluation Systems}: Building synthetic data and metrics
                \item \textbf{Segmentation}: Identifying high-impact improvement areas
                \item \textbf{Specialized Retrieval}: Building domain-specific tools
                \item \textbf{Feedback Collection}: Designing for continuous improvement
            \end{itemize}
            \item Practical takeaways you can implement immediately:
            \begin{itemize}
                \item How to generate synthetic test data for your domain
                \item Setting up basic retrieval evaluation metrics
                \item Techniques for query segmentation and analysis
                \item Simple feedback mechanisms to start collecting data
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Interactive Discussion Plan}
        \begin{itemize}
            \item \textbf{Quick Poll}: Where are you in your RAG journey?
            \begin{itemize}
                \item Data collection/preparation
                \item Basic retrieval implementation
                \item Evaluation and measurement
                \item Specialized tools and routing
                \item User feedback and fine-tuning
            \end{itemize}
            \item \textbf{Key Questions to Consider}:
            \begin{itemize}
                \item How do you measure success for your specific use case?
                \item What are your biggest retrieval challenges?
                \item How specialized does your domain knowledge need to be?
                \item What feedback mechanisms make sense for your users?
            \end{itemize}
            \item We'll adjust focus based on your responses
        \end{itemize}
    \end{frame}
} % End of studentunderstandingframe command 